{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_punctuation\n",
    "import pickle as pkl\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip punctuation and remove stopwords\n",
    "def preprocess(sentence_list):\n",
    "    filters = [lambda x : x.lower(), strip_punctuation, remove_stopwords]\n",
    "    sentence_token=preprocess_string(''.join(sentence_list),filters)\n",
    "    return sentence_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Word2Vector model\n",
    "def w2v_training( total_corpus, embedding_size):\n",
    "    min_word_count = 2                      \n",
    "    num_workers = 4       \n",
    "    context = 5                                                                                       \n",
    "\n",
    "    print(\"Train gensim word2vector...\")\n",
    "    w2vModel = gensim.models.Word2Vec(total_corpus, size=embedding_size, window=context, min_count=min_word_count, workers=num_workers)\n",
    "    w2vModel.save('/Users/Hannah/ML_project/Enron/trained_word2vector')\n",
    "    return w2vModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build embedding matrix for word2vector or Fasttext\n",
    "def build_embedding(input_model, total_corpus, word_index, EMBEDDING_SIZE):\n",
    "    path='/Users/Hannah/ML_project/Enron/'\n",
    "    num_words = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_SIZE))\n",
    "    \n",
    "    if input_model=='trained_word2vector':\n",
    "        model=w2v_training(total_corpus, EMBEDDING_SIZE)\n",
    "        for word, i in word_index.items():\n",
    "            if word in model.wv.vocab:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = model.wv[word]\n",
    "        np.save(path+'trained_word2vector_embedding_matrix.npy', embedding_matrix)          \n",
    "    \n",
    "    elif input_model=='Fasttext' :\n",
    "        if 'fasettext_embedding_index.npy' not in [f for f in os.listdir(path)]:\n",
    "            print('Build embedding matrix...')\n",
    "            fasttest_path=path+'crawl-300d-2M.vec'\n",
    "            embeddings_index = dict()\n",
    "            f = open(fasttest_path,encoding='utf-8')\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs \n",
    "            f.close()\n",
    "            for word, i in word_index.items():\n",
    "                if word in embeddings_index.keys():\n",
    "                    embedding_matrix[i] = embeddings_index.get(word)\n",
    "            \n",
    "            \n",
    "            np.save(path+'fasettext_embedding_matrix.npy', embedding_index)\n",
    "                    \n",
    "        else:\n",
    "            print('Load embedding matrix...')\n",
    "            embedding_matrix=np.load(path+'fasettext_embedding_matrix.npy')\n",
    "        \n",
    "    return embedding_matrix, num_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "path='/Users/Hannah/ML_project/Enron/data/'\n",
    "dataset=[]\n",
    "class_name=[] \n",
    "count_labels=[]  #number of texts in each class\n",
    "for i, names in enumerate(os.listdir(path)):   \n",
    "    class_name.append(names)\n",
    "    count_labels.append([i]*len(os.listdir(path+names+'/text/')))\n",
    "    for texts in os.listdir(path+names+'/text/'):\n",
    "        with open(path+names+'/text/'+texts) as file:\n",
    "            dataset.append(file.readlines())   \n",
    "labels=list(itertools.chain(*count_labels)) #flatten labels\n",
    "labels_index=dict([(class_name[x], x) for x in range(len(class_name))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Corpus\n",
    "total_corpus=[]\n",
    "for sentence_list in dataset:\n",
    "    total_corpus.append(preprocess(sentence_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Flatten, Reshape, LSTM, Dense, Embedding, merge, Dropout, dot, Activation, Bidirectional,GRU\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize corpus\n",
    "MAX_NB_WORDS=45000\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='UNK', num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(total_corpus)\n",
    "sequences = tokenizer.texts_to_sequences(total_corpus)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose fasttext or word2vec as embedding\n",
    "embedding_matrix, num_words= build_embedding('Fasttext', total_corpus,  word_index,  EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set max sequence length\n",
    "MAX_SEQUENCE_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad sequences to same length\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot labels\n",
    "one_hot_labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', one_hot_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to training set and validation\n",
    "X_train, X_valid, y_train, y_valid =train_test_split(data,  one_hot_labels ,stratify = one_hot_labels, test_size=0.3, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define evaluation metrics\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    \n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use class weight to deal with imblance data\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(labels), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings into an Embedding layer\n",
    "# Set trainable = False so as to keep the embeddings fixed\n",
    "\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_SIZE,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "# Bideractional GRU\n",
    "sequence_input=Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x=Bidirectional(GRU(128, return_sequences=True, recurrent_dropout=0.1),merge_mode=\"sum\")(embedded_sequences)\n",
    "x=Dropout(0.5)(x)\n",
    "x=Bidirectional(GRU(128, return_sequences=True, recurrent_dropout=0.1),merge_mode=\"sum\")(x)\n",
    "x=Dropout(0.5)(x)\n",
    "x=Flatten()(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[recall_m, precision_m, f1_m])\n",
    "\n",
    "model.fit(X_train, y_train, class_weight=class_weights,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
