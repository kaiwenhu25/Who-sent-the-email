{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, os.path\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_punctuation\n",
    "import pickle as pkl\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip punctuation and remove stopwords\n",
    "def preprocess(sentence_list):\n",
    "    filters = [lambda x : x.lower(), strip_punctuation, remove_stopwords]\n",
    "    sentence_token=preprocess_string(''.join(sentence_list),filters)\n",
    "    return sentence_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build topic model \n",
    "def topic_model(total_corpus, NUM_TOPICS):\n",
    "    dictionary = gensim.corpora.Dictionary(total_corpus)\n",
    "    doc2bow_corpus = [dictionary.doc2bow(words) for words in total_corpus]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(doc2bow_corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    answer = []\n",
    "    for doc in doc2bow_corpus:\n",
    "        answer.append(ldamodel.get_document_topics(doc))\n",
    "    matrix = []\n",
    "    for line in answer:\n",
    "        weight_list = []\n",
    "        for i in range(NUM_TOPICS):\n",
    "            tup = [item for item in line if i in item]\n",
    "            if len(tup) == 0:\n",
    "                    weight_list.append(0)\n",
    "            else:\n",
    "                weight_list.append(float(tup[0][1]))\n",
    "        matrix.append(weight_list)\n",
    "    matrix = np.asarray(matrix) \n",
    "    return matrix\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build matrix of topic models for texts and subjects\n",
    "def build_topic_matrix(model, total_corpus, NUM_TOPICS):\n",
    "    save_path='/Users/Hannah/ML_project/Enron/'\n",
    "    if model=='texts_topic':\n",
    "        if 'topic_matrix_50.npy' not in [f for f in os.listdir(save_path)]:\n",
    "            print('Train text topic model...')\n",
    "            matrix=topic_model(total_corpus,  NUM_TOPICS)\n",
    "            np.save(save_path+'topic_matrix_50.npy', matrix)\n",
    "        else:\n",
    "            print('Load text topic model...')\n",
    "            matrix= np.load(save_path+'topic_matrix_50.npy')\n",
    "            \n",
    "    elif model=='subject_topic':\n",
    "        if 'subject_topic_10_matrix.npy' not in [f for f in os.listdir(save_path)]:\n",
    "            print('Train subject topic model...')\n",
    "            matrix=topic_model(total_corpus, NUM_TOPICS)\n",
    "            np.save(save_path+'subject_topic_matrix.npy', matrix)\n",
    "        else:\n",
    "            print('Load subject topic model...')\n",
    "            matrix= np.load(save_path+'subject_topic_10_matrix.npy')\n",
    "\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many sequences are in a text\n",
    "def sequence_num(docs):\n",
    "    return [[len(sent_tokenize(x))] for x in docs]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many token with stopwords are in a text\n",
    "def token_num(docs):\n",
    "    return [[len( word_tokenize(x))] for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many token without stopwords are in a text\n",
    "def token_num_wo_stopword(docs):\n",
    "    seq=[preprocess(x) for x in docs]\n",
    "    return [[len(words)] for words in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many special characters are in a text\n",
    "def special_char_num(dataset):\n",
    "    ave_s_char_num=[]\n",
    "    s_char_num=[]\n",
    "    for seq in dataset:\n",
    "        text=' '.join(seq)\n",
    "        \n",
    "        st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
    "              \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "        count = 0\n",
    "        for i in text:\n",
    "            if (i in st):\n",
    "                count = count + 1\n",
    "        ave_s_char_num.append([count / len(text)])\n",
    "        s_char_num.append([count])\n",
    "    \n",
    "    return ave_s_char_num, s_char_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many puncuation characters are in a text\n",
    "def puncuation_char_num(dataset):\n",
    "    ave_p_char_num=[]\n",
    "    p_char_num=[]\n",
    "    for seq in dataset:\n",
    "        text=' '.join(seq)\n",
    "        st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
    "        count = 0\n",
    "        for i in text:\n",
    "            if (i in st):\n",
    "                count = count + 1\n",
    "        ave_p_char_num.append([count / len(text)])\n",
    "        p_char_num.append([count])\n",
    "    \n",
    "    return ave_p_char_num, p_char_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many functional words are in a text\n",
    "def functional_words(docs):\n",
    "    functional_words = \"\"\"a between in nor some upon\n",
    "    about both including nothing somebody us\n",
    "    above but inside of someone used\n",
    "    after by into off something via\n",
    "    all can is on such we\n",
    "    although cos it once than what\n",
    "    am do its one that whatever\n",
    "    among down latter onto the when\n",
    "    an each less opposite their where\n",
    "    and either like or them whether\n",
    "    another enough little our these which\n",
    "    any every lots outside they while\n",
    "    anybody everybody many over this who\n",
    "    anyone everyone me own those whoever\n",
    "    anything everything more past though whom\n",
    "    are few most per through whose\n",
    "    around following much plenty till will\n",
    "    as for must plus to with\n",
    "    at from my regarding toward within\n",
    "    be have near same towards without\n",
    "    because he need several under worth\n",
    "    before her neither she unless would\n",
    "    behind him no should unlike yes\n",
    "    below i nobody since until you\n",
    "    beside if none so up your\n",
    "    \"\"\"\n",
    "    sum_functional_words=[]\n",
    "    ave_functional_words=[]\n",
    "    \n",
    "    functional_words = functional_words.split()\n",
    "    \n",
    "    for text in docs:\n",
    "        words = strip_punctuation(text)\n",
    "        count = 0\n",
    "        for i in word_tokenize(words):\n",
    "            if i in functional_words:\n",
    "                count += 1\n",
    "        ave_functional_words.append([count / (len(words)+1)])\n",
    "        sum_functional_words.append([count])\n",
    "    return ave_functional_words, sum_functional_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many non-repeating words are in a text\n",
    "def set_word(docs):\n",
    "    ave_set_words=[]\n",
    "    set_words=[]\n",
    "    for text in docs:\n",
    "        words = strip_punctuation(text)\n",
    "        token = word_tokenize(words)\n",
    "        set_words .append([len(set(token))])\n",
    "\n",
    "    return set_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load subjects and texts \n",
    "path='/Users/Hannah/ML_project/Enron/data/'\n",
    "\n",
    "dataset=[]\n",
    "class_name=[] \n",
    "count_labels=[]  \n",
    "subject_dataset=[]\n",
    "for i, names in enumerate(os.listdir(path)):   \n",
    "    class_name.append(names)\n",
    "    count_labels.append([i]*len(os.listdir(path+names+'/text/')))\n",
    "    for texts in os.listdir(path+names+'/text/'):\n",
    "        with open(path+names+'/text/'+texts) as file:\n",
    "            title='.'.join(texts.split('.')[:2])\n",
    "            dataset.append(file.readlines())   \n",
    "        with open(path+names+'/subject/'+title+'.JavaMail.evans@thyme.subject') as subject_file:\n",
    "            subject_dataset.append(subject_file.readlines())  \n",
    "        \n",
    "labels=list(itertools.chain(*count_labels)) \n",
    "num_labels=[len(x) for x in count_labels]\n",
    "accum_labels=np.cumsum([len(x) for x in count_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build corpus\n",
    "docs=[]\n",
    "for sentence_list in dataset:\n",
    "    docs.append(' '.join(sentence_list))\n",
    "    \n",
    "total_corpus=[]\n",
    "for sentence_list in dataset:\n",
    "    total_corpus.append(preprocess(sentence_list))\n",
    "\n",
    "subject_total_corpus=[]\n",
    "for sentence_list in subject_dataset:\n",
    "    subject_total_corpus.append(preprocess(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text topic model...\n",
      "Training subject topic model...\n"
     ]
    }
   ],
   "source": [
    "#Build Topic models for texts and subjects\n",
    "NUM_TEXT_TOPICS=50\n",
    "NUM_SUBJECT_TOPICS=10\n",
    "\n",
    "    \n",
    "topic_50_matrix=build_topic_matrix('texts_topic',total_corpus, NUM_TEXT_TOPICS)\n",
    "subject_topic_50_matrix=build_topic_matrix('subject_topic',subject_total_corpus, NUM_SUBJECT_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate statistics about the texts\n",
    "seq_num = sequence_num(docs)\n",
    "tok_num = token_num(docs)\n",
    "tok_num_wo_stopw = token_num_wo_stopword(docs)\n",
    "ave_s_char_num, s_char_num=special_char_num(dataset)\n",
    "ave_p_char_num, p_char_num=puncuation_char_num(dataset)\n",
    "ave_functional_words, sum_functional_words =functional_words(docs)\n",
    "set_words=set_word(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine statistics\n",
    "X=np.concatenate((seq_num, tok_num, tok_num_wo_stopw, ave_s_char_num, s_char_num, ave_p_char_num, p_char_num,\n",
    "                ave_functional_words, sum_functional_words, set_words, topic_50_matrix, subject_topic_50_matrix), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to training set and validation\n",
    "X_train_feature, X_valid_feature, y_train, y_valid = train_test_split(X, labels, test_size=0.3, stratify=labels, random_state=66)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(docs, labels, test_size=0.3, stratify=labels, random_state=66)\n",
    "\n",
    "#Calculate TFIDF\n",
    "tfidf_vectorizer= TfidfVectorizer( max_features=3000, min_df=3, max_df=0.7, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer .fit_transform(X_train)\n",
    "X_valid_tfidf=tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "#Comnine training set and validation\n",
    "X_train_all=np.concatenate((X_train_feature,X_train_tfidf.toarray()), axis=1)\n",
    "X_valid_all=np.concatenate((X_valid_feature,X_valid_tfidf.toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap the data in dmatrix\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set params\n",
    "params = {\n",
    "\"objective\":'multi:softmax',\n",
    "'num_class': 148,\n",
    "\"booster\" : \"dart\",\n",
    "\"eval_metric\": \"auc\",\n",
    "\"eta\": 0.04,\n",
    "\"tree_method\": 'exact',\n",
    "\"max_depth\": 7,\n",
    "\"subsample\": 0.07,\n",
    "\"colsample_bytree\": 0.8,\n",
    "\"silent\": 0,\n",
    "\"alpha\" :0.01,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build XGBoost classifier\n",
    "xgb_clf = xgb.train(params, dtrain, num_boost_round=30)\n",
    "y_pred = xgb_clf.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use f1 score as the evaluation metrics\n",
    "f1_score(y_valid, y_pred, average='micro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the classification report\n",
    "print(classification_report(y_valid, y_pred, target_names=class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
